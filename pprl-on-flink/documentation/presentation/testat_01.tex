\documentclass{beamer}

\usepackage[utf8]{inputenc}

\title{PPRL on Flink}
\author{Thomas Hornoff, Martin Franke}

\begin{document}
	
	\maketitle	

	\begin{frame}
		\frametitle{Datensatz}
		
		\begin{itemize}
			\item Nutzung der North Carolina Voter Registration Database
				\begin{itemize}
					\item Erreichbar unter \url{http://dl.ncsbe.gov/}
					\item Verschieden große Datensätze
					\item txt-Dateien Tab-separiert
					\item county code, first name, last name, middle name, name suffix, residential
					 adress, state, city, zip, race code, ethnic code, gender code, age, party code,
					 dmv date 
				\end{itemize}
			
			\item Nutzung eines DataCorruptors\footnote{P. Christen and D. Vatsalan: Flexible
			 and extensible generation and corruption of personal data.} zur Erstellung von
			 Teilmengen des Datensatzes sowie zur Modifikation der Daten (Dirty Data) und 
			 eventuell zum Einfügen von IDs.
		\end{itemize}	
	
	\end{frame}



    \begin{frame}
    		\frametitle{Architektur}
    		
		\begin{itemize}
			\item Data Owner / Parteien
				\begin{itemize}
					\item Einigung über Paramter (Länge der N-Gramme, Anzahl Hashfunktionen,
					Größe der Bloom Filter, ...
					\item Halten verschiedene Datensätze
					\item Codierung der Records aus ihren Datensätze (Erstellung der Bloom Filter)
					\item Blocking mittels LSH-MinHash 
					(? Wo soll das Blocking durchgeführt werden ?)
					\item Senden der Daten an die Linkage Unit
				\end{itemize}
			\item Linkage Unit
				\begin{itemize}
					\item Blocking mittels LSH-MinHash (?)
					\item Durchführung des Linkage Algorithmus
					\item Senden der Ergebnisse an die Data Owner / beteiligten Parteien
				\end{itemize}	
		\end{itemize}    		  
    \end{frame}



    \begin{frame}
    		\frametitle{Ablauf}  

		\begin{itemize}
			\item Datenimport: Lesen des Voter-Files und Übertragung der relevanten Felder in
			 ein DataSet
			\begin{itemize}
				\item Vorname
				\item Nachname
				\item Adresse
				\item Geburtsdatum
			\end{itemize}
			\item Datenvorverarbeitung: Konkatenation und Normalisierung der relevanten
			 Felder. Eventuell Einfügung von künstlichen IDs.
			\item Erstellung N-Gramme: Überführung der konkatenierten QIDs in N-Gramme mit
			 Hilfe einer FlatMap-Funktion.
			\item Zusammenfassung der N-Gramme: Zusammenfassung aller N-Gramme des
			 gleichen Records mit Hilfe einer Reduce-Funktion. (Eventuell zsfg. mit vorherigem
			  Schritt?)
			\item Erstellung der Bloom Filter: Aufnahme aller N-Gramme eines Records in einen
			 Bloom Filter (Map-Funktion).
		\end{itemize}
		
    \end{frame}

    \begin{frame}
    		\frametitle{Ablauf (Fortsetzung)}
    
    		\begin{itemize}
        		\item Blocking: Erstellung von Blöcken von Kandidat-Record-Paaren mit Hilfe von
        		 LSH-MinHash. (FlatMap / Reduce)
			\item Linkage I: Vergleich aller Kandidaten-Record-Paare eines Blocks mit Hilfe einer
			 Ähnlichkeitsfunktion (FlatMap).
			\item Linkage II: Vergleich aller Ähnlichkeitswerte mit dem Schwellwert. Behalte nur
			 die Werte mit einem hinreichend guten Wert. (Reduce)
			\item Bekanntgabe der Ergebnisse: Speicherung oder Versand der Matching Pairs.
			 (Paar von IDs)
		\end{itemize}
    \end{frame}


\end{document}
