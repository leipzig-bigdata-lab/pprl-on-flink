\documentclass[10pt]{article}

\usepackage{url}
%\usepackage{graphicx}
%\usepackage{cite}
%\usepackage{tabularx}

%\newcommand{\rom}[1]{\MakeUppercase{\romannumeral #1}}

\usepackage{listings}
%\usepackage[ruled,vlined,linesnumbered,nofillcomment]{algorithm2e}
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\ttfamily\tiny,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Java,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\begin{document}

    \input{./titlepage.tex}

    \section{Introduction}

        In order to fulfill the first assignment of the \textit{Big Data Praktikum} three main topics have to be
        implemented. Firstly, the system has to be set up, i.e. Apache Flink has to run on a computer system. Secondly,
        the planned architecture for the implementation of PPRL with Flink has to be outlined. Thirdly, an
        implementation plan has to be described for future work.
        In the following sections all three issues are discussed.

    \section{Installation}

        In this section we will shortly outline setting up the Flink on a machine in order to test it and
        getting a first impression of the framework.

        \subsection{Setting up \& testing Flink}

        \textit{Apache Flink}\cite{ApacheFlink} is downloadable from the Apache website \url{https://flink.apache.org}. The installation
            is straight forward and therefore does not need any explanation here, since there is a very good
            documentation on the \textit{Apache Flink} website.

            % Setting up test environment -> include java code
            In case of testing the Bloom Filter we decided to use the Bloom Filter by \textit{Baqend}\cite{Baqend}. Furthermore,
            we used \textit{Debatty}\cite{Debatty} as a framwork for LSH. Both functions will be implemented by ourself, since
            our purposes will be different.

        \subsection{Dataset}

            % test data from Sehili        
            % hadoop hdfs

        The \textit{North Carolina Voter Registratin Databse}\cite{NorthCarolinaVoterRegistration}
            supports our puposes to test
            simple implementations of the Bloom-Filter and the LSH. This Dataset is one of the few
            which includes real names, addresses, as well as the age of these people. Since we want to encrypt this data
            it is highly important that we use such data for PPRL with Flink.

            Furthermore, the use of a \textit{DataCorruptor} constructing subsets of the dataset as well as
            modifying data is preferable. Implementing a corruptor as described in \cite{DataCorruptor}, and thus transforming
            it into Java is another issue of our goal on PPRL with Flink.

            % short explanatin of both datasets -> which data is relevant? -> .txt and .vcs data

    \section{Architecture}

        % using hamming distance!

        The data owner will represent our data, i.e. the North Carolina Voter Registration and the
        given csv-files given by Sehili. We will use the first name, last name, address, and date of birth.
        Both, the length of the Bloom-Filter input and its token length must be flexible. That is, the data is encoded
        via the Bloom-Filter which then solves as input to the Lnkage Unit.
        The Linkage Unit uses LSH-MinHashes in order to block the data. The LSH-MinHashes will use the Hamming-Distance
        to claculate the LSH. After blocking the data, the Linkage algorithm calculates the desired outcome,
        which is sent to the data owner. That concludes the cycle of PPRL with Flink.

    \section{Implementation Plan}

        % implementing own bloom-filter (flatmap->flatmap->reduce [in parallel] )
        % implementing own LSH -> keys or subkeys

        In order to implement our architecture we will first have to read the Voter-files and
        extract the relevant data, i.e. first name, last name, address, and date of birth.
        We then have to generate N-Grams. Therefore, we have to use the FlatMap function
        in order to transfer the concatinated QIDs into N-Grams. N-Grams of the same record will then be
        combined using a Reduce-function.
        Having reduced the N-Grams of one record, Bloom-Filter will be calculated subsequently.

        The Linkage Unit will then block these records and create blocks of Candidate-Record-Pairs
        using a LSH-MinHash-function. That means that the FlatMap and Reduce functions will be used in
        the order 'FlatMap->FlatMap->Reduce'.
        That means, the first Linkage in the Linkage Unit all Candidates-Record-Pairs of one block are
        being compared with the help of the FlatMap function.
        The second linkage will compare all Candidates-Record-Pairs of a block using the
        FlatMap function as well. The final and third linkage will then compare all
        Candidates-Record-Pairs of a block using the Reduce-function.
        Finally, the results are stored for further examitnation by the data owners.

%\begin{lstlisting}
%\end{lstlisting}


        \bibliographystyle{plain}
        \bibliography{library}

\end{document}
